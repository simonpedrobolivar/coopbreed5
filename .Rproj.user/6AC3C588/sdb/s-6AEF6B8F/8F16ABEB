{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Functions and parallelised for-loops in R\"\nauthor: \"Severin Hauenstein\"\ndate: \"30 November 2016\"\noutput:\n  html_document:\n    toc: yes\n    theme: readable\n\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE, fig.width = 5, fig.height = 5)\npar(pty = \"s\")\n```\n\n## Introduction\nR-functions are sub-programs. They contain code chunks, which we frequently use in a very similar fashion, but do not want re-write every time. \n\nWhen handling and analysing data in R we make use of the growing number of built-in functions, which are stored in freely available R-packages^[https://cran.r-project.org/]. \nDespite the activeness of the R-community, user-specific problems, for which we do not find any built-in function, occur frequently. This may be because there is simply no function available, or because of insufficient flexibility of a roughly suitable built-in function. \n\nLuckily we can increase our flexibility by writing our own user-defined R-functions. In the first part of this tutorial the `how to' will be demonstrated on several examples.\n\n### Basic structure of R-functions\n\n```{r, eval = FALSE}\n# function name              # function head   \nname.of.function <- function(argument_1, argument_2, ...){\n  computation on the arguments ...\n  ... and potentially some other code\n  return(results)\n}\n```\n\nAll R-functions exhibit the same basic structure, and are written in the familiar R syntax. \nIt all starts with a function name, which is assigned to the function in the same way we assign objectnames to objects. \nThe function-head contains none, one or multiple function arguments. These are variables that are made available to the local function envrionment. Objects in the global envrionment (also known as workspace) are not part of this local environment and thus not available to the function statements. \nThe statements are computations on the arguments and/or additional code, implemented to achieve the desired result(s).\nThe result(s) can then be returned to the script or global environment using the `return()` function, which automatically closes the local function environment (and frees memory used within the local function enviornment).\nCode appearing after `return()` will not be executed.\n\n*N.B.*, it is good practise to assign meaningful names to the function and its arguments, such that other users ideally grasp the functionality by intuition.\n\n\n\n### Example 01: Re-programming simple built-in functions\n\nLet's get used to the basic function construct by re-programming simple built-in functions. \nTo compute the sum or the mean of a vector of numeric values using a simple calculator, we have to add all values indivdually (and divide it by the number of values for the mean). \nIn R, we use the `base`-package function `sum()` and `mean()`, respectively, which do the calculations iteratively for us. \nTo illustrate the structure of simple R-functions we will now re-programme these two functions.\n\n#### Example 01.1: Re-programming `base::sum()`\n\nOur first function's name will be `sum_of_x`, because it calculates the sum of a numeric vector `x`, which is the first argument of `sum_of_x()`. The function contains a second argument, `removeNAs = FALSE`, whose functionality will become clear in just a moment. Notice that the second argument defaults to `FALSE`, whereas `x` does not correspond to a value or object by default. \nIn the core of the function we define the following statements:\n\n1. Prior to any calculation, check that `x` is in fact a numeric object. If not, we stop the function and return an error message. This is not essential, yet a user-defined error message can help to debug the function. \n2. Create an output object (here called `out`) and set it to zero\n3. Start a for-loop with `length(x)` iterations `i`\n  + iteratively add `x[i]` to `out`\n  + if the second function argument `removeNAs = TRUE`, we check whether `x[i]` is NA. If so, we disregard it by adding 0 to `out`.\n4. return `out` as the result of the function\n\n\n```{r}\nsum_of_x <- function(x, removeNAs = FALSE){\n  # make sure that argument x is numeric\n  if(!is.numeric(x)) stop(\"x must be a numeric vector\")\n  \n  out <- 0 # initiate output, set to zero\n  # for-loop to iterate through input argument x\n  for(i in seq_along(x)){ \n    # increment out with i-th element of x\n    out <- out + \n      # when na.rm = TRUE, NA values are turned into zeros\n      ifelse(removeNAs, ifelse(is.na(x[i]), 0, x[i]), x[i])\n  }\n  \n  # return out \n  return(out)\n}\n```\n\nLet's test our function on a random sample of integer values. We can check the validity of the result by doing the calculation with the `sum()` function in the `base`-package, or simply in our head.\n\n```{r}\n(vector <- sample(10, size = 5)) # sample from 5 values from [1;10]\nsum_of_x(x = vector) # our sum function\nbase::sum(vector) # built-in sum function\n```\n\n\n#### Example 01.2: Re-programming `base::mean()`\n\nTo calculate the mean of a numeric vector we will make use of `sum_of_x()`.\nNote that the function structure is very similar. The only difference is that we do not create an intermediary output object `out`, but do the calculations inside the `return` command. This is not essential but saves some memory, as well as a line of code.\n\n```{r}\nmean_of_x <- function(x, removeNAs = FALSE){\n  # make sure that argument x is numeric\n  if(!is.numeric(x)) stop(\"x must be a numeric vector\")\n  \n  # calculate mean inside the return command\n  return(sum_of_x(x, removeNAs = removeNAs) / \n           # divide the result of our sum_of_x function by the length of x\n           # if removeNAs = TRUE we divide the sum by the length of x without NAs\n           ifelse(removeNAs, sum(!is.na(x)), length(x))) \n}\n\n```\n\n```{r}\n(vector <- sample(10, size = 5)) # sample from 5 values from [1;10]\nmean_of_x(x = vector) # our mean function\nbase::mean(vector) # built-in mean function\n```\n\n### Example 02: Data management\nNow let's get to an actual user-specific problem for which there isn't already a built-in function readily available. \nThis is often the case when managing data in R.\nLet's assume you want to fit a statistical model in the Bayesian framework. You likely do that with jags. Now, the function `R2jags::jags()` requires the data as a list instead of the dataframe we are used to work with. \nWhen fitting many models and especially when dealing with many covariates, it can be tedious to manually reorganise a dataframe to a list with proper names every time. \n\nSo, let's write a function that does that for us:\n\n```{r}\n# usually our data looks like this, a data frame with the variables as columns and observations as rows\nhead(trees)\n\n# transform this data frame to a list\ndf2list <- function(df, ...){ # the ... is known as ellipsis (and also as dot dot dot)\n  \n  # append the list of covariates in the data frame, and the ellipsis objects\n  out <- append(lapply(seq(NCOL(df)), function(x) df[,x]), list(...)) \n  # add the covariates's names \n  names(out)[1:NCOL(df)] <- colnames(df) \n  \n  return(out)  # return the list\n}\n\n```\n\n```{r}\n# test function on trees and cars\ndf2list(trees, N=NROW(trees))\ndf2list(cars, N=NROW(cars))\n```\n\n\n## Parallelised for-loops\nWe use for-loops to iterate through data, which is an absolute necessity for data management, as well as analysis. However, with the data becoming larger and larger, for-loops exhibit a computational bottleneck, as they are extremly slow in R. \n\nTo a certain extent there is a work around to this problem: parallelisation. \nFor this we need a multi-core processor is needed, as well as a sufficient amout of memory. By dividing out a computation among cores (also known as workers) the used memory multiplies by the number of involved workers.\nNormally R runs on a single core of the processor. To exhaust the available computational resources of a computer we need explicitly parallelise our tasks. \n\n\nIn the following second part of this tutorial, I will demonstrate how to parallelise for-loops on a simple and a more advanced example. \n\n### Example 03: simple parallelised for-loops\nIn the simple example we calculate the mean value for each column in a matrix. There is a built-in R-function for this, called `colMeans()`. We forget about that one for a moment. Instead we are going to iterate through the columns and use our own `mean_of_x()` function for the calculation of the column mean.\n\n```{r}\n# create a matrix with 10 columns, each with 1000000 values drawn from a standard uniform distribution\nmat <- matrix(runif(10000000), ncol = 10)\n```\n\n\n#### normal (unparallelised) for-loop\n```{r}\n# calculate the column means with the mean_of_x() function from example 01.2 in a normal for-loop\ncolumn.means1 <- numeric(10)\n\nsystem.time(\n  for(i in 1:10){\n  column.means1[i] <- mean_of_x(mat[,i])\n  }\n)\n\n# print result\ncolumn.means1\n\n```\n\n#### parallelised for-loop\n\n```{r}\n# load required packages\nlibrary(foreach)\nlibrary(doParallel)\n\n# start timing\nptm <- proc.time() \n\n# use all cores available, except 1\ncores <- parallel::detectCores() - 1\n\n# set up cluster\ncl <- parallel::makeCluster(cores)\ndoParallel::registerDoParallel(cl)\n  \n# start parallelised for-loop\ncolumn.means2 <- foreach::foreach(i = 1:NCOL(mat), .combine = c) %dopar%{\n  mean_of_x(mat[,i])\n}\n\n# stop cluster, free memory form workers\nparallel::stopCluster(cl = cl) \n\n# stop timing\nproc.time() - ptm \n\n# result\ncolumn.means2\n\n```\n\nNotice that the time saved by the parallelised for-loop in this example is not substantial. The time to set up a computation cluster is an additional task, which takes time. Yet, this influence on the end time becomes smaller with increasing computational effort of the main task.\n\n### Example 04: parallelised bootstrap\nBootstrapping is a resampling method, mainly used to obtain standard error estimates for (non-parametric) statistical models.\nIt is an iterative process, and it needs many iterations. Since the individual models runs are independant of each other, bootstrapping make a good example where parallelisation can reduce the computation effort substantially (depending of the available computational infrastructure, and the computation effort of the model).\n\nEven though it is most often used to estimate the standard errors for non-parametric approaches, I will demonstrate parallelised bootstrapping on a logistic regression analysis. \nWe will use simulated data for this. Imagine we record presence/absence of cougars (*Puma concolor*) and the respective distance forest edge, and we expect an optimum for a certain distance to the forest edge. \nThese assumptions provide the basis for the following data simulation and analysis.\n\n\n#### Data simulation\n```{r}\n# data simulation \nset.seed(011216) # Set a random seed for reproducibility of the simulation\n\nsamplesize <- 500 # Number of data points\ndistToForest <- sort(rnorm(samplesize, 0, 1)) # draw predictor values from standard Gaussian distribution\n\n# Drawing presence/absence from Bernoulli distribution, with probs as a function of distance to forest\npresAbs <- rbinom(samplesize, size = 1, prob = plogis(1 - 0.2 * distToForest - 0.3*distToForest^2)) \n\ncougar <- data.frame(distToForest = distToForest, presAbs = presAbs)\nhead(cougar)\n```\n\n#### Model fitting\nFirst we fit a Bernoulli GLM to the data following the hypothesis of a quadratic relationship of distance to forest and the presence of cougars.\n```{r}\n# logistic regression\nfm <- glm(presAbs ~ distToForest + I(distToForest^2), data = cougar, family = binomial)\n```\n\n\n#### Bootstrap\nThe next step is to bootstrap the above described model, which means that the model is repeatedly fit with re-sampled data. This has to be done in a loop. To speed up the process we can simply divide the `Nboot` bootstrap iterations among the available `r parallel::detectCores() - 1` cores. \nWithin the loop we use the bootstrapped models to make predictions on a newly defined equidistant sequence within the range of the recorded distance to forest values. \n```{r, cache = TRUE}\n# bootstrap iterations\nNboot <- 5000\n\n# new sequence of b_length, to predict for\nnewDTF <- seq(min(cougar$distToForest), max(cougar$distToForest), len = 500)\n\n# start timing\nptm <- proc.time() \n\n# use all cores available, except 1\ncores <- parallel::detectCores() - 1\n\n# set up cluster\ncl <- parallel::makeCluster(cores)\ndoParallel::registerDoParallel(cl)\n  \n# start parallelised for-loop\nbootresults <- foreach::foreach(i = 1:Nboot, .combine = cbind) %dopar%{\n  \n  # non-stratified re-sampling\n  bootsample <- sample(1:nrow(cougar), replace=T)\n  \n  # re-fit model to re-sampled data (bootSample)\n  bootfm <- update(fm, data = cougar[bootsample, ])\n  \n  # make predictions for new data\n  predict(bootfm, newdata = data.frame(\"distToForest\" = newDTF), type = \"response\")\n}\n\n# stop cluster, free memory form workers\nparallel::stopCluster(cl = cl) \n\n# stop timing\nproc.time() - ptm \n\n```\n\n\n#### Visualisation\nTo visualise the results of the little analysis, we can plot the regression line with the bootstrapped 95% confidence intervals. We get those by simple calculation of the 2.5% and 97.5% quantiles of the bootstrap predictions. \n```{r}\n# plot data\nwith(cougar, plot(presAbs ~ distToForest, xlab = \"distance to forest\", ylab = \"presence/absence\"))\n\n# compute quantiles\nqs <- apply(bootresults, 1, quantile, probs=c(0.025, 0.5, 0.975)) \n# plot mean prediction and 95% confidence intervall\nlines(qs[1,] ~ newDTF, lwd=2, lty=2, col=rgb(0.545,0,0,0.8))\nlines(qs[2,] ~ newDTF, lwd=2, lty=1, col=rgb(0.545,0,0,0.8))\nlines(qs[3,] ~ newDTF, lwd=2, lty=2, col=rgb(0.545,0,0,0.8))\n\n# mark x where density of bootstrapped predictions is plotted next\nlines(rep(newDTF[40],2), c(qs[1,40], qs[3,40])); text(newDTF[40], qs[3,40]+0.04, labels = 1)\nlines(rep(newDTF[250],2), c(qs[1,250], qs[3,250])); text(newDTF[250], qs[3,250]+0.04, labels = 2)\n```\n\nWe may also have a look at the distribution or density of the bootstrap predictions for two specific values of distance to forest (marked in the figure above as black lines labeled 1 and 2).\n```{r, fig.width = 10}\n#plot density of bootstrapped predictions at previously marked x\npar(mfrow=c(1,2))\nplot(density(bootresults[40,]), main = 1) \nplot(density(bootresults[250,]), main = 2)\npar(mfrow=c(1,1))\n```\n\n\n## A parallelised bootstrap R-function\nThe last part of this tutorial is a combination of the two first parts. We will write an R-function that does parallelised bootstrapping for us. Keep in mind that you may have to increase the flexibility of the following function for your own use. In this example the functionality will be restricted to GLM-like model structures, and only one predictor for which the predictions are made. This function will not be very useful, it merely serves the purpose of combining all features demonstrated in this tutorial. \n\n```{r}\nbootstrapping <- function(model, data, newx, predictorName, Nboot = 5000, \n                          parallel = FALSE, plot.effect = TRUE, \n                          progressbar = TRUE, ...){\n  \n  # start timing\n  ptm <- proc.time()\n  \n  # set up progress bar\n  if(progressbar){\n    pb <- txtProgressBar(min = 1, max = Nboot, style = 3)\n    progress <- function(n) setTxtProgressBar(pb, n)\n    opts <- list(progress = progress)\n  }\n  \n  \n  # if not parallel\n  if(!parallel){\n    bootresults <- matrix(NA, nrow = length(newx), ncol = Nboot)\n    for(i in seq(Nboot)){\n      # non-stratified re-sampling\n      bootsample <- sample(1:nrow(data), replace=T)\n      \n      # re-fit model to re-sampled data (bootSample)\n      bootfm <- update(model, data = data[bootsample, ])\n      \n      # make predictions on new data using bootfm\n      newdf <- data.frame(newx)\n      colnames(newdf) <- predictorName\n      bootresults[,i] <- predict(bootfm, newdata = newdf, type = \"response\")\n      if(progressbar) setTxtProgressBar(pb, i) # progress bar\n    }\n  }else{ # if parallel\n    # load necessary packages\n    library(foreach)\n    library(doSNOW)\n    \n    # parallel can be TRUE or auto, then all available cores are used, except one\n    if(parallel == T | parallel == \"auto\"){\n      cores <- parallel::detectCores() - 1\n      message(\"parallel, set cores automatically to \", cores)\n    }else if (is.numeric(parallel)){ # number of cores can be manually defined\n      cores <- parallel\n      message(\"parallel, set number of cores manually to \", cores)\n    }else stop(\"wrong argument to parallel\")\n    \n    # set up computation cluster\n    cl <- parallel::makeCluster(cores)\n    doSNOW::registerDoSNOW(cl)\n    \n    # start parallelised for-loop\n    bootresults <- foreach::foreach(i = 1:Nboot, .combine = cbind, \n                                    .options.snow= ifelse(progressbar, opts, list())) %dopar%{\n  \n      # non-stratified re-sampling\n      bootsample <- sample(1:nrow(data), replace=T)\n      \n      # re-fit model to re-sampled data (bootSample)\n      bootfm <- update(model, data = data[bootsample, ])\n      \n      # make predictions on new data using bootfm\n      newdf <- data.frame(newx)\n      colnames(newdf) <- predictorName\n      predict(bootfm, newdata = newdf, type = \"response\")\n    }\n\n    # stop cluster, free memory form workers\n    parallel::stopCluster(cl = cl) \n    \n  }\n  \n  # close progress bar connection\n  if(progressbar) close(pb)\n  \n  # compute quantiles\n  qs <- apply(bootresults, 1, quantile, probs=c(0.025, 0.5, 0.975)) \n  \n  if(plot.effect){\n    par(...)\n    # plot data\n    plot(model$terms[1])\n    # plot mean prediction and 95% confidence intervall\n    lines(qs[1,] ~ newx, lwd=2, lty=2, col=rgb(0.545,0,0,0.8))\n    lines(qs[2,] ~ newx, lwd=2, lty=1, col=rgb(0.545,0,0,0.8))\n    lines(qs[3,] ~ newx, lwd=2, lty=2, col=rgb(0.545,0,0,0.8))\n  }\n  \n  results <- vector(\"list\", length = 7)\n  results$model <- model\n  results$data <- data\n  results$newdata <- newx\n  results$iterations <- Nboot\n  results$predictions <- bootresults\n  results$quantiles <- qs\n  \n  # stop timing\n  print(proc.time() - ptm )\n  \n  return(results)\n}\n```\n\nLet's test the function on the simulated cougar dataset for a binomial glm, one time with `parallel = TRUE`, and one time with `parallel = FALSE`.\n```{r}\nbootGlm <- bootstrapping(model = fm, data = cougar, newx = newDTF, \n                         predictorName = \"distToForest\", Nboot = 5000, \n                         parallel = TRUE, plot.effect = TRUE, progressbar = FALSE)\n\nbootGlmSlow <- bootstrapping(model = fm, data = cougar, newx = newDTF, \n                             predictorName = \"distToForest\", Nboot = 5000, \n                             parallel = FALSE, plot.effect = TRUE, progressbar = FALSE)\n\n```\n\n\n\n",
    "created" : 1492592312387.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2917723532",
    "id" : "8F16ABEB",
    "lastKnownWriteTime" : 1480497856,
    "last_content_update" : 1480497856,
    "path" : "~/Dokumente/R_projects/Functions and parallel for-loops in R/r_functionsAndParallelLoops.rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}